{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2fc269",
   "metadata": {},
   "source": [
    "# Training DIPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d35d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-glibc2.17\n",
      "Python 3.10.9 (main, Mar  8 2023, 10:47:38) [GCC 11.2.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 12:12:11.337471: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.10.0\n",
      "GPU Resources Available:\n",
      "\t []\n",
      "Keras Version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train dipz with keras\n",
    "\"\"\"\n",
    "#'Take only this many inputs (with no args %(const)s)'\n",
    "_h_take_first = 'Take only this many inputs (with no args %(const)s)'\n",
    "\n",
    "import sys\n",
    "import getopt\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import platform\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Python {sys.version}\")\n",
    "\n",
    "# local libs\n",
    "from layers import Sum\n",
    "from utils import gaussian_loss\n",
    "from utils import TRANSFORMS\n",
    "from utils import scale\n",
    "from utils import renamed\n",
    "from utils import build_feature\n",
    "from utils import get_gaussian_loss_prec\n",
    "\n",
    "# mlearnin libs\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPU Resources Available:\\n\\t\",gpus)\n",
    "\n",
    "from tensorflow import keras\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import (\n",
    "    Dense, TimeDistributed, Input, Concatenate, Masking\n",
    ")\n",
    "from keras.utils.generic_utils import CustomMaskWarning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the data libs\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "# random python utility libs\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afce0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: False\n",
      "Eval: True\n",
      "[12:18:24] Start\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "Mean probability density (higher is better): 0.7984\n",
      "Mean absolute z-score (lower is better): 0.8118\n",
      "Mean squared difference z-score (lower is better): 0.4612\n",
      "[12:18:25] Runtime: 0.556s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2023)\n",
    "keras.utils.set_random_seed(2023)\n",
    "\n",
    "# Set these manually to run either in the notebook\n",
    "NOTEBOOK_TRAIN = False\n",
    "NOTEBOOK_EVAL = True\n",
    "\n",
    "# DATA_FILEPATH = \"./data/data.h5\"\n",
    "DATA_FILEPATH = \"./data/subset.h5\"    \n",
    "\n",
    "model_name = \"regress\" # Original config file\n",
    "TRAIN = False\n",
    "EVAL = False\n",
    "args = sys.argv[1:]\n",
    "opts, args = getopt.getopt(args, \"m:f:te\", [\"model=\", \"train\", \"eval\"])\n",
    "for opt, arg in opts:\n",
    "    if opt in ['-m', '--model']:\n",
    "        model_name = arg\n",
    "    elif opt in ['-t', '--train']:\n",
    "        TRAIN = True\n",
    "    elif opt in ['-e', '--eval']:\n",
    "        EVAL = True\n",
    "    elif opt in ['-f']:\n",
    "        TRAIN = NOTEBOOK_TRAIN\n",
    "        EVAL = NOTEBOOK_EVAL\n",
    "\n",
    "print(f\"Train: {TRAIN}\")\n",
    "print(f\"Eval: {EVAL}\")\n",
    "\n",
    "CONFIG_FILEPATH = f\"./config/{model_name}.json\"\n",
    "OUTPUT_FILEPATH = f\"./models/{model_name}\"\n",
    "\n",
    "# TODO: clean up these hardcoded values\n",
    "MASK_VALUE = 999\n",
    "MERGED_NODES = [32]*4\n",
    "\n",
    "# A function to define and gets the config file \n",
    "def get_config(config_path):\n",
    "    with open(config_path) as cfg:\n",
    "        config = json.load(cfg)\n",
    "    return dict(\n",
    "        jetfeatnames=config[\"jetfeatnames\"],\n",
    "        trackfeatnames=config[\"trackfeatnames\"],\n",
    "        targetfeatnames=config[\"targetfeatnames\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        epoch_size=config[\"epoch_size\"],\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        learning_rate=config[\"lr\"],\n",
    "        tracknodes=config['tracknodes'],\n",
    "        jetnodes=config['jetnodes'],\n",
    "    )\n",
    "\n",
    "# A function that defines and gets the neural network model\n",
    "def get_model(config, mask_value):\n",
    "    n_track_inputs = len(config['trackfeatnames'])\n",
    "    track_inputs = Input(shape=(None,n_track_inputs))\n",
    "\n",
    "    n_jet_inputs = len(config['jetfeatnames'])\n",
    "    jet_inputs = Input(shape=(n_jet_inputs))\n",
    "\n",
    "    # add jet layers\n",
    "    x = jet_inputs\n",
    "    for nodes in config['jetnodes']:\n",
    "        x = Dense(units=nodes, activation='relu')(x)\n",
    "    jet_latent = x\n",
    "\n",
    "    # add track layers\n",
    "    x = track_inputs\n",
    "    x = Masking(mask_value=mask_value)(x)\n",
    "    for nodes in config['tracknodes']:\n",
    "        x = TimeDistributed(Dense(nodes, activation='relu'))(x)\n",
    "    x = Sum()(x)\n",
    "    track_latent = x\n",
    "\n",
    "    # merge the layers\n",
    "    merged = Concatenate()([jet_latent, track_latent])\n",
    "    # todo: not clear how many additonal processing layers we should\n",
    "    # add here\n",
    "    x = merged\n",
    "    for nodes in MERGED_NODES:\n",
    "        x = Dense(nodes, activation='relu')(x)\n",
    "    out_latent = x\n",
    "    outputs = keras.layers.Dense(units=2)(out_latent)\n",
    "    model = keras.Model(\n",
    "        inputs=[jet_inputs, track_inputs],\n",
    "        outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                  loss=gaussian_loss)\n",
    "    return model\n",
    "\n",
    "# A function that imports the dataset we will be working on\n",
    "def get_dataset(h5_filepath, config, mask_value, take_first=False):\n",
    "    \"\"\"\n",
    "    We make some hardcoded transformations to normalize these inputs\n",
    "    \"\"\"\n",
    "\n",
    "    # pt is log transformed\n",
    "    # Z0 is divided by 50\n",
    "    # target is divided by 50\n",
    "\n",
    "    trf = TRANSFORMS\n",
    "    # identy function to pass through things that aren't listed above\n",
    "    def ident(x):\n",
    "        return x\n",
    "\n",
    "    sl = slice(None,None,None)\n",
    "    if take_first:\n",
    "        sl = slice(0,take_first,None)\n",
    "\n",
    "    with h5py.File(h5_filepath) as h5file:\n",
    "        # get track array\n",
    "        td = h5file['fs_tracks_simple_ip']\n",
    "        tfn = config['trackfeatnames']\n",
    "        # we can pass through NaNs here\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            trackstack = [trf.get(x,ident)(td[x,sl,...]) for x in tfn]\n",
    "        track_array = np.stack(trackstack, axis=2)\n",
    "        invalid = np.isnan(td['pt',sl])\n",
    "        track_array[invalid,:] = mask_value\n",
    "\n",
    "        # get jet array\n",
    "        jd = h5file['jets']\n",
    "        jfn = config['jetfeatnames']\n",
    "        jetstack = [trf.get(x,ident)(jd[x,sl]) for x in jfn]\n",
    "        jet_array = np.stack(jetstack, axis=1)\n",
    "\n",
    "        # get targets\n",
    "        tfn = config['targetfeatnames']\n",
    "        targetstack = [trf.get(x,ident)(jd[x,sl]) for x in tfn]\n",
    "        target_array = np.stack(targetstack, axis=1)\n",
    "\n",
    "    return jet_array, track_array, target_array\n",
    "\n",
    "# A function that gets the inputs to save them\n",
    "def get_inputs(jet_feature_names, track_feature_names):\n",
    "    track_variables = [build_feature(x) for x in track_feature_names]\n",
    "    jet_variables = [build_feature(x) for x in jet_feature_names]\n",
    "    return {\n",
    "        'input_sequences': [\n",
    "            {\n",
    "                'name': 'tracks_loose202102NoIpCuts_absD0DescendingSort',\n",
    "                'variables': track_variables,\n",
    "            }\n",
    "        ],\n",
    "        'inputs': [\n",
    "            {\n",
    "                'name': 'btagging',\n",
    "                'variables': jet_variables\n",
    "            }\n",
    "        ],\n",
    "        'outputs': [\n",
    "            {\n",
    "                'labels': ['z','negLogSigma2'],\n",
    "                'name': 'dipz'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# A function that saves the model\n",
    "def save_model(model, output_dir, inputs):\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    with open(output_dir / 'architecture.json', 'w') as arch:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=CustomMaskWarning)\n",
    "            arch.write(model.to_json(indent=2))\n",
    "\n",
    "    model.save_weights(output_dir / 'weights.h5')\n",
    "\n",
    "    with open(output_dir / 'inputs.json', 'w') as inputs_file:\n",
    "        json.dump(inputs, inputs_file, indent=2)\n",
    "\n",
    "def split_data(jet_inputs, track_inputs, targets):\n",
    "    assert jet_inputs.shape[0] == track_inputs.shape[0] == targets.shape[0]\n",
    "\n",
    "    return train_test_split(jet_inputs, track_inputs, targets,\n",
    "                            train_size = 0.75, random_state=2023)\n",
    "\n",
    "# def eval(model, )\n",
    "\n",
    "# A function that runs the neural network training and saves the weights\n",
    "def run(config_filepath, h5_filepath, num_epochs = 10):\n",
    "    mask_value = MASK_VALUE\n",
    "    config = get_config(config_filepath)\n",
    "    model = get_model(config, mask_value=mask_value)\n",
    "    jet_inputs, track_inputs, targets = get_dataset(h5_filepath, config, mask_value)\n",
    "    jet_inputs_train, jet_inputs_test, track_inputs_train, track_inputs_test, targets_train, targets_test = \\\n",
    "        split_data(jet_inputs, track_inputs, targets)\n",
    "    \n",
    "    if TRAIN:\n",
    "        print()\n",
    "        print(f\"----- Training Model: {model_name} -----\", end=\"\\n\\n\", flush=True)\n",
    "        stopping_callback = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                        patience=10,\n",
    "                                                        verbose=1,\n",
    "                                                        start_from_epoch=100)\n",
    "        model.fit([jet_inputs_train, track_inputs_train], targets_train,\n",
    "                batch_size=config[\"batch_size\"],\n",
    "                epochs=num_epochs,\n",
    "                callbacks=[stopping_callback])\n",
    "        inputs = get_inputs(config['jetfeatnames'], config['trackfeatnames'])\n",
    "        save_model(model, inputs=inputs, output_dir=Path(OUTPUT_FILEPATH))\n",
    "    else:\n",
    "        model.load_weights(Path(OUTPUT_FILEPATH) / 'weights.h5')\n",
    "\n",
    "    if EVAL:\n",
    "        print()\n",
    "        print(f\"----- Evaluating Model: {model_name} -----\", end=\"\\n\\n\", flush=True)\n",
    "        # Using RMSE for now\n",
    "        pred = model.predict([jet_inputs_test, track_inputs_test])\n",
    "        z = pred[:,0]\n",
    "        zhat = targets_test[:,0]\n",
    "\n",
    "        # Not sure what to do with these\n",
    "        sigma = np.sqrt(np.exp(-pred[:,1])) # Standard deviation\n",
    "        from scipy.stats import norm\n",
    "\n",
    "        pdf_score = norm.pdf(z, loc=zhat, scale=sigma)\n",
    "        pdf_result = np.mean(pdf_score)\n",
    "        print(f\"Mean probability density (higher is better): {str(round(pdf_result, 4))}\")\n",
    "\n",
    "        z_score = np.abs(z - zhat) / sigma\n",
    "        z_result = np.mean(z_score)\n",
    "        print(f\"Mean absolute z-score (lower is better): {str(round(z_result, 4))}\")\n",
    "\n",
    "        z_square_score = np.square(z - zhat) / sigma\n",
    "        z_square_result = np.mean(z_square_score)\n",
    "        print(f\"Mean squared difference z-score (lower is better): {str(round(z_square_result, 4))}\")\n",
    "\n",
    "\n",
    "BEGIN = time.time()\n",
    "print(f\"[{datetime.datetime.now().strftime(f'%H:%M:%S')}] Start\", flush=True)\n",
    "\n",
    "run(CONFIG_FILEPATH, DATA_FILEPATH, num_epochs=1000)\n",
    "\n",
    "print(f\"[{datetime.datetime.now().strftime(f'%H:%M:%S')}]\", end=\" \")\n",
    "print(f\"Runtime: {round(time.time() - BEGIN, 3)}s\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
