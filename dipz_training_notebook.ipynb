{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2fc269",
   "metadata": {},
   "source": [
    "# Training DIPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d35d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train dipz with keras\n",
    "\"\"\"\n",
    "#'Take only this many inputs (with no args %(const)s)'\n",
    "_h_take_first = 'Take only this many inputs (with no args %(const)s)'  \n",
    "\n",
    "# TODO: clean up these hardcoded values\n",
    "MASK_VALUE = 999\n",
    "MERGED_NODES = [32]*4\n",
    "\n",
    "# local libs\n",
    "from layers import Sum\n",
    "from utils import gaussian_loss\n",
    "from utils import TRANSFORMS\n",
    "from utils import scale\n",
    "from utils import renamed\n",
    "from utils import build_feature\n",
    "from utils import get_gaussian_loss_prec\n",
    "\n",
    "# mlearnin libs\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, TimeDistributed, Input, Concatenate, Masking\n",
    ")\n",
    "from keras.utils.generic_utils import CustomMaskWarning\n",
    "\n",
    "# the data libs\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "# random python utility libs\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# A function to define and gets the config file \n",
    "def get_config(config_path):\n",
    "    with open(config_path) as cfg:\n",
    "        config = json.load(cfg)\n",
    "    return dict(\n",
    "        jetfeatnames=config[\"jetfeatnames\"],\n",
    "        trackfeatnames=config[\"trackfeatnames\"],\n",
    "        targetfeatnames=config[\"targetfeatnames\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        epoch_size=config[\"epoch_size\"],\n",
    "        number_epochs=config[\"number_epochs\"],\n",
    "        learning_rate=config[\"lr\"],\n",
    "        tracknodes=config['tracknodes'],\n",
    "        jetnodes=config['jetnodes'],\n",
    "    )\n",
    "\n",
    "# A function that defines and gets the neural network model\n",
    "def get_model(config, mask_value):\n",
    "    n_track_inputs = len(config['trackfeatnames'])\n",
    "    track_inputs = Input(shape=(None,n_track_inputs))\n",
    "\n",
    "    n_jet_inputs = len(config['jetfeatnames'])\n",
    "    jet_inputs = Input(shape=(n_jet_inputs))\n",
    "\n",
    "    # add jet layers\n",
    "    x = jet_inputs\n",
    "    for nodes in config['jetnodes']:\n",
    "        x = Dense(units=nodes, activation='relu')(x)\n",
    "    jet_latent = x\n",
    "\n",
    "    # add track layers\n",
    "    x = track_inputs\n",
    "    x = Masking(mask_value=mask_value)(x)\n",
    "    for nodes in config['tracknodes']:\n",
    "        x = TimeDistributed(Dense(nodes, activation='relu'))(x)\n",
    "    x = Sum()(x)\n",
    "    track_latent = x\n",
    "\n",
    "    # merge the layers\n",
    "    merged = Concatenate()([jet_latent, track_latent])\n",
    "    # todo: not clear how many additonal processing layers we should\n",
    "    # add here\n",
    "    x = merged\n",
    "    for nodes in MERGED_NODES:\n",
    "        x = Dense(nodes, activation='relu')(x)\n",
    "    out_latent = x\n",
    "    outputs = keras.layers.Dense(units=2)(out_latent)\n",
    "    model = keras.Model(\n",
    "        inputs=[jet_inputs, track_inputs],\n",
    "        outputs=outputs)\n",
    "    # print the summary\n",
    "    model.summary()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                  loss=gaussian_loss)\n",
    "    return model\n",
    "\n",
    "# A function that imports the dataset we will be working on\n",
    "def get_dataset(h5file_path, config, mask_value, take_first=False):\n",
    "    \"\"\"\n",
    "    We make some hardcoded transformations to normalize these inputs\n",
    "    \"\"\"\n",
    "\n",
    "    # pt is log transformed\n",
    "    # Z0 is divided by 50\n",
    "    # target is divided by 50\n",
    "\n",
    "    trf = TRANSFORMS\n",
    "    # identy function to pass through things that aren't listed above\n",
    "    def ident(x):\n",
    "        return x\n",
    "\n",
    "    sl = slice(None,None,None)\n",
    "    if take_first:\n",
    "        sl = slice(0,take_first,None)\n",
    "\n",
    "    with h5py.File(h5file_path) as h5file:\n",
    "        # get track array\n",
    "        td = h5file['fs_tracks_simple_ip']\n",
    "        tfn = config['trackfeatnames']\n",
    "        # we can pass through NaNs here\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            trackstack = [trf.get(x,ident)(td[x,sl,...]) for x in tfn]\n",
    "        track_array = np.stack(trackstack, axis=2)\n",
    "        invalid = np.isnan(td['pt',sl])\n",
    "        track_array[invalid,:] = mask_value\n",
    "\n",
    "        # get jet array\n",
    "        jd = h5file['jets']\n",
    "        jfn = config['jetfeatnames']\n",
    "        jetstack = [trf.get(x,ident)(jd[x,sl]) for x in jfn]\n",
    "        jet_array = np.stack(jetstack, axis=1)\n",
    "\n",
    "        # get targets\n",
    "        tfn = config['targetfeatnames']\n",
    "        targetstack = [trf.get(x,ident)(jd[x,sl]) for x in tfn]\n",
    "        target_array = np.stack(targetstack, axis=1)\n",
    "\n",
    "    return jet_array, track_array, target_array\n",
    "\n",
    "\n",
    "# A function that gets the inputs to save them\n",
    "def get_inputs(jet_feature_names, track_feature_names):\n",
    "    track_variables = [build_feature(x) for x in track_feature_names]\n",
    "    jet_variables = [build_feature(x) for x in jet_feature_names]\n",
    "    return {\n",
    "        'input_sequences': [\n",
    "            {\n",
    "                'name': 'tracks_loose202102NoIpCuts_absD0DescendingSort',\n",
    "                'variables': track_variables,\n",
    "            }\n",
    "        ],\n",
    "        'inputs': [\n",
    "            {\n",
    "                'name': 'btagging',\n",
    "                'variables': jet_variables\n",
    "            }\n",
    "        ],\n",
    "        'outputs': [\n",
    "            {\n",
    "                'labels': ['z','negLogSigma2'],\n",
    "                'name': 'dipz'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# A function that saves the model\n",
    "def save_model(model, output_dir, inputs):\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    with open(output_dir / 'architecture.json', 'w') as arch:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=CustomMaskWarning)\n",
    "            arch.write(model.to_json(indent=2))\n",
    "\n",
    "    model.save_weights(output_dir / 'weights.h5')\n",
    "\n",
    "    with open(output_dir / 'inputs.json', 'w') as inputs_file:\n",
    "        json.dump(inputs, inputs_file, indent=2)\n",
    "        \n",
    "\n",
    "# A function that runs the neural network training and saves the weights\n",
    "def run(num_epochs = 10):\n",
    "    mask_value = MASK_VALUE\n",
    "    config = get_config(\"../regress.json\")\n",
    "    model = get_model(config, mask_value=mask_value)\n",
    "    jet_inputs, track_inputs, targets = get_dataset(\n",
    "    \"../user.viruelas.27383479._000001.output.h5\", config, mask_value)\n",
    "    model.fit([jet_inputs, track_inputs], targets, epochs=num_epochs)\n",
    "    inputs = get_inputs(config['jetfeatnames'], config['trackfeatnames'])\n",
    "    save_model(model, inputs=inputs, output_dir=Path('outputs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afce0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None, 8)]    0           []                               \n",
      "                                                                                                  \n",
      " masking_1 (Masking)            (None, None, 8)      0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDistri  (None, None, 16)    144         ['masking_1[0][0]']              \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 32)           96          ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDistri  (None, None, 16)    272         ['time_distributed_4[0][0]']     \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 32)           1056        ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDistri  (None, None, 16)    272         ['time_distributed_5[0][0]']     \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 32)           1056        ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " time_distributed_7 (TimeDistri  (None, None, 16)    272         ['time_distributed_6[0][0]']     \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 32)           1056        ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " sum_1 (Sum)                    (None, 16)           0           ['time_distributed_7[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 48)           0           ['dense_16[0][0]',               \n",
      "                                                                  'sum_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 32)           1568        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 32)           1056        ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 32)           1056        ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 32)           1056        ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 2)            66          ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,026\n",
      "Trainable params: 9,026\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -0.5853\n",
      "Epoch 2/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -2.0571\n",
      "Epoch 3/100\n",
      "18173/18173 [==============================] - 26s 1ms/step - loss: -2.7593\n",
      "Epoch 4/100\n",
      "18173/18173 [==============================] - 26s 1ms/step - loss: -2.6396\n",
      "Epoch 5/100\n",
      "18173/18173 [==============================] - 26s 1ms/step - loss: -3.3370\n",
      "Epoch 6/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -3.3572\n",
      "Epoch 7/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -3.7741\n",
      "Epoch 8/100\n",
      "18173/18173 [==============================] - 26s 1ms/step - loss: -3.8867\n",
      "Epoch 9/100\n",
      "18173/18173 [==============================] - 24s 1ms/step - loss: -3.8573\n",
      "Epoch 10/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -3.9193\n",
      "Epoch 11/100\n",
      "18173/18173 [==============================] - 26s 1ms/step - loss: -4.0562\n",
      "Epoch 12/100\n",
      "18173/18173 [==============================] - 29s 2ms/step - loss: -3.7717\n",
      "Epoch 13/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -3.9804\n",
      "Epoch 14/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -3.9655\n",
      "Epoch 15/100\n",
      "18173/18173 [==============================] - 26s 1ms/step - loss: -4.0256\n",
      "Epoch 16/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -4.1264\n",
      "Epoch 17/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.1082\n",
      "Epoch 18/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -3.9142\n",
      "Epoch 19/100\n",
      "18173/18173 [==============================] - 24s 1ms/step - loss: -4.1770\n",
      "Epoch 20/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.0405\n",
      "Epoch 21/100\n",
      "18173/18173 [==============================] - 24s 1ms/step - loss: -4.0163\n",
      "Epoch 22/100\n",
      "18173/18173 [==============================] - 24s 1ms/step - loss: -4.1378\n",
      "Epoch 23/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -4.0514\n",
      "Epoch 24/100\n",
      "18173/18173 [==============================] - 24s 1ms/step - loss: -4.1520\n",
      "Epoch 25/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.1090\n",
      "Epoch 26/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.1163\n",
      "Epoch 27/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.0894\n",
      "Epoch 28/100\n",
      "18173/18173 [==============================] - 24s 1ms/step - loss: -4.1812\n",
      "Epoch 29/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -4.0812\n",
      "Epoch 30/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -4.2850\n",
      "Epoch 31/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.0793\n",
      "Epoch 32/100\n",
      "18173/18173 [==============================] - 25s 1ms/step - loss: -4.2561\n",
      "Epoch 33/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.3704\n",
      "Epoch 34/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.3262\n",
      "Epoch 35/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1856\n",
      "Epoch 36/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3106\n",
      "Epoch 37/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.3386\n",
      "Epoch 38/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2730\n",
      "Epoch 39/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1174\n",
      "Epoch 40/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.2207\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3654\n",
      "Epoch 42/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3323\n",
      "Epoch 43/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.4104\n",
      "Epoch 44/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.3923\n",
      "Epoch 45/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2004\n",
      "Epoch 46/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3439\n",
      "Epoch 47/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2711\n",
      "Epoch 48/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.4482\n",
      "Epoch 49/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3818\n",
      "Epoch 50/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3759\n",
      "Epoch 51/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3635\n",
      "Epoch 52/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2847\n",
      "Epoch 53/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3901\n",
      "Epoch 54/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2723\n",
      "Epoch 55/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2929\n",
      "Epoch 56/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2765\n",
      "Epoch 57/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3311\n",
      "Epoch 58/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3359\n",
      "Epoch 59/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.4056\n",
      "Epoch 60/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3637\n",
      "Epoch 61/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.0262\n",
      "Epoch 62/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2017\n",
      "Epoch 63/100\n",
      "18173/18173 [==============================] - 23s 1ms/step - loss: -4.1923\n",
      "Epoch 64/100\n",
      "18173/18173 [==============================] - 24s 1ms/step - loss: -4.2582\n",
      "Epoch 65/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1800\n",
      "Epoch 66/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2050\n",
      "Epoch 67/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3083\n",
      "Epoch 68/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3007\n",
      "Epoch 69/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2693\n",
      "Epoch 70/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2948\n",
      "Epoch 71/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3606\n",
      "Epoch 72/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2288\n",
      "Epoch 73/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1465\n",
      "Epoch 74/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1831\n",
      "Epoch 75/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3072\n",
      "Epoch 76/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2390\n",
      "Epoch 77/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2487\n",
      "Epoch 78/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2111\n",
      "Epoch 79/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2514\n",
      "Epoch 80/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2904\n",
      "Epoch 81/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1574\n",
      "Epoch 82/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3181\n",
      "Epoch 83/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3374\n",
      "Epoch 84/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.0341\n",
      "Epoch 85/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.0737\n",
      "Epoch 86/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3035\n",
      "Epoch 87/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2796\n",
      "Epoch 88/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3628\n",
      "Epoch 89/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: 48.7441\n",
      "Epoch 90/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1151\n",
      "Epoch 91/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1465\n",
      "Epoch 92/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1513\n",
      "Epoch 93/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3614\n",
      "Epoch 94/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3426\n",
      "Epoch 95/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3897\n",
      "Epoch 96/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2833\n",
      "Epoch 97/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -3.9756\n",
      "Epoch 98/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.3032\n",
      "Epoch 99/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.1431\n",
      "Epoch 100/100\n",
      "18173/18173 [==============================] - 22s 1ms/step - loss: -4.2059\n"
     ]
    }
   ],
   "source": [
    "run(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
